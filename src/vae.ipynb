{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch import Tensor\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from coral_pytorch.losses import corn_loss\n",
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "from torchmetrics.functional import concordance_corrcoef\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class MultiCategoricalLoss:\n",
    "    n_values: int\n",
    "    n_classes: int\n",
    "    is_categorical: bool\n",
    "    is_ordinal: bool\n",
    "    weight: torch.Tensor = None\n",
    "\n",
    "    @property\n",
    "    def last_layer_size(self) -> int:\n",
    "        if self.is_categorical and self.is_ordinal:\n",
    "            raise ValueError(\"Cannot be both categorical and ordinal.\")\n",
    "\n",
    "        if self.is_categorical:\n",
    "            return self.n_values * self.n_classes\n",
    "        elif self.is_ordinal:\n",
    "            return self.n_values * (self.n_classes - 1)\n",
    "        else:\n",
    "            return self.n_values\n",
    "\n",
    "    def calculate_reconstruction(self, y_raw: torch.Tensor) -> torch.LongTensor:\n",
    "        if self.is_categorical:\n",
    "            return y_raw.reshape((-1, self.n_values, self.n_classes)).argmax(dim=-1)\n",
    "        elif self.is_ordinal:\n",
    "            return corn_label_from_logits(\n",
    "                y_raw.reshape(-1, self.n_classes - 1)\n",
    "            ).reshape((-1, self.n_values))\n",
    "        else:\n",
    "            return (F.sigmoid(y_raw) * (self.n_classes - 1)).round().long()\n",
    "\n",
    "    def calculate_reconstruction_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "        if self.is_categorical:\n",
    "            y_pred = y_pred.reshape((-1, self.n_values, self.n_classes))\n",
    "            return F.cross_entropy(\n",
    "                input=y_pred.reshape(-1, self.n_classes),\n",
    "                target=y_true.reshape(-1).long(),\n",
    "                weight=self.weight,\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        elif self.is_ordinal:\n",
    "            y_pred = y_pred.reshape((-1, self.n_values, self.n_classes - 1))\n",
    "            return corn_loss(\n",
    "                logits=y_pred.reshape(-1, self.n_classes - 1),\n",
    "                y_train=y_true.reshape(-1).long(),\n",
    "                num_classes=self.n_classes,\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                F.mse_loss(\n",
    "                    input=F.sigmoid(y_pred),\n",
    "                    target=y_true / (self.n_classes - 1),\n",
    "                    reduction=\"none\",\n",
    "                )\n",
    "                .sum(axis=-1)\n",
    "                .mean()\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class VariationalAutoencoderOutput:\n",
    "    z_dist: torch.distributions.Distribution\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.LongTensor\n",
    "\n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_generative: torch.Tensor\n",
    "\n",
    "    def log(self, logger, batch, prefix: str):\n",
    "        logger.log(f\"{prefix}_loss\", self.loss)\n",
    "        logger.log(f\"{prefix}_loss_recon\", self.loss_recon)\n",
    "        logger.log(f\"{prefix}_loss_generative\", self.loss_generative)\n",
    "\n",
    "        # Log evaluation metrices\n",
    "        logger.log(\n",
    "            f\"{prefix}_rel_wrong_items\",\n",
    "            ((batch != self.x_recon).sum(axis=-1) / batch.shape[-1]).median(),\n",
    "        )\n",
    "        real_sum = batch.sum(axis=-1)\n",
    "        reconstructed_sum = self.x_recon.sum(axis=-1)\n",
    "        logger.log(\n",
    "            f\"{prefix}_concordance\",\n",
    "            concordance_corrcoef(\n",
    "                target=real_sum.to(torch.float), preds=reconstructed_sum.to(torch.float)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def calculate_total_error(self, batch):\n",
    "        real_sum = batch.sum(axis=-1)\n",
    "        reconstructed_sum = self.x_recon.sum(axis=-1)\n",
    "        return F.l1_loss(input=reconstructed_sum, target=real_sum, reduction=\"mean\")\n",
    "\n",
    "\n",
    "class GenerativeLoss:\n",
    "    def __call__(self, dist: torch.distributions.Distribution, z: Tensor):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class KullbackLeiblerLoss(GenerativeLoss):\n",
    "    def __init__(self, beta: float = 1.0):\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(self, dist: torch.distributions.Distribution, z: Tensor):\n",
    "        std_normal = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros_like(dist.mean, device=dist.mean.device),\n",
    "            scale_tril=torch.eye(dist.mean.shape[-1], device=dist.mean.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(dist.mean.shape[0], -1, -1),\n",
    "        )\n",
    "        return self.beta * torch.distributions.kl.kl_divergence(dist, std_normal).mean()\n",
    "\n",
    "\n",
    "class WassersteinLoss(GenerativeLoss):\n",
    "    def __init__(self, reg_weight: float, kernel_type: str, z_var: float):\n",
    "        self.reg_weight = reg_weight\n",
    "        self.kernel_type = kernel_type\n",
    "        self.z_var = z_var\n",
    "        \n",
    "    def __call__(self, dist: torch.distributions.Distribution, z: Tensor):\n",
    "        # Calculate the corrected reg_weight\n",
    "        batch_size = z.size(0)\n",
    "        bias_corr = batch_size * (batch_size - 1)\n",
    "        reg_weight = self.reg_weight / bias_corr\n",
    "\n",
    "        # Sample from prior (Gaussian) distribution\n",
    "        prior_z = torch.randn_like(z)\n",
    "\n",
    "        prior_z__kernel = self.compute_kernel(prior_z, prior_z)\n",
    "        z__kernel = self.compute_kernel(z, z)\n",
    "        priorz_z__kernel = self.compute_kernel(prior_z, z)\n",
    "\n",
    "        mmd = (\n",
    "            reg_weight * prior_z__kernel.mean()\n",
    "            + reg_weight * z__kernel.mean()\n",
    "            - 2 * reg_weight * priorz_z__kernel.mean()\n",
    "        )\n",
    "        return mmd\n",
    "\n",
    "    def compute_kernel(self, x1: Tensor, x2: Tensor) -> Tensor:\n",
    "        # Convert the tensors into row and column vectors\n",
    "        D = x1.size(1)\n",
    "        N = x1.size(0)\n",
    "\n",
    "        x1 = x1.unsqueeze(-2)  # Make it into a column tensor\n",
    "        x2 = x2.unsqueeze(-3)  # Make it into a row tensor\n",
    "\n",
    "        x1 = x1.expand(N, N, D)\n",
    "        x2 = x2.expand(N, N, D)\n",
    "\n",
    "        if self.kernel_type == \"rbf\":\n",
    "            result = self.compute_rbf(x1, x2)\n",
    "        elif self.kernel_type == \"imq\":\n",
    "            result = self.compute_inv_mult_quad(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(\"Undefined kernel type.\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_rbf(self, x1: Tensor, x2: Tensor, eps: float = 1e-7) -> Tensor:\n",
    "        z_dim = x2.size(-1)\n",
    "        sigma = 2.0 * z_dim * self.z_var\n",
    "\n",
    "        result = torch.exp(-((x1 - x2).pow(2).mean(-1) / sigma))\n",
    "        return result\n",
    "\n",
    "    def compute_inv_mult_quad(\n",
    "        self, x1: Tensor, x2: Tensor, eps: float = 1e-7\n",
    "    ) -> Tensor:\n",
    "        z_dim = x2.size(-1)\n",
    "        C = 2 * z_dim * self.z_var\n",
    "        kernel = C / (eps + C + (x1 - x2).pow(2).sum(dim=-1))\n",
    "\n",
    "        # Exclude diagonal elements\n",
    "        result = kernel.sum() - kernel.diag().sum()\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: Sequence[int], dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i, (i_size, out_size) in enumerate(zip(layers, layers[1:])):\n",
    "            if i < len(layers) - 2:\n",
    "                self.encoder.append(nn.Linear(i_size, out_size))\n",
    "                if dropout > 0.0:\n",
    "                    self.encoder.append(nn.Dropout(dropout))\n",
    "                self.encoder.append(nn.SiLU())\n",
    "            else:\n",
    "                self.encoder.append(\n",
    "                    nn.Linear(i_size, 2 * out_size)\n",
    "                )  # 2 for mean and variance.\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, eps: float = 1e-8) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        return mu, scale\n",
    "\n",
    "    def forward_and_reparameterize(\n",
    "        self, x\n",
    "    ) -> Tuple[torch.distributions.Distribution, torch.Tensor]:\n",
    "        mu, scale = self(x)\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        dist = torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "        return dist, dist.rsample()\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: Sequence[int], dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential()\n",
    "        for i, (i_size, out_size) in enumerate(zip(layers, layers[1:])):\n",
    "            self.decoder.append(nn.Linear(i_size, out_size))\n",
    "            if i < len(layers) - 2:\n",
    "                if dropout > 0.0:\n",
    "                    self.decoder.append(nn.Dropout(dropout))\n",
    "                self.decoder.append(nn.SiLU())\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reconstruction_loss: MultiCategoricalLoss,\n",
    "        generative_loss: GenerativeLoss,\n",
    "        intermediate_layers: Sequence[int],\n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.reconstruction_loss = reconstruction_loss\n",
    "        self.generative_loss = generative_loss\n",
    "        self.encoder = Encoder([reconstruction_loss.n_values] + intermediate_layers, dropout=dropout)\n",
    "        self.decoder = Decoder(\n",
    "            list(reversed(intermediate_layers)) + [reconstruction_loss.last_layer_size], dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        dist, z = self.encoder.forward_and_reparameterize(x)\n",
    "        recon_x = self.decoder(z)\n",
    "\n",
    "        if not compute_loss:\n",
    "            return VariationalAutoencoderOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=self.reconstruction_loss.calculate_reconstruction(recon_x),\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_generative=None,\n",
    "            )\n",
    "\n",
    "        loss_recon = self.reconstruction_loss.calculate_reconstruction_loss(\n",
    "            y_true=x, y_pred=recon_x\n",
    "        )\n",
    "        loss_generative = self.generative_loss(dist, z)\n",
    "\n",
    "        loss = loss_recon + loss_generative\n",
    "        return VariationalAutoencoderOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=self.reconstruction_loss.calculate_reconstruction(recon_x),\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_generative=loss_generative,\n",
    "        )\n",
    "\n",
    "\n",
    "class VariationAutoencoderModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reconstruction_loss: MultiCategoricalLoss,\n",
    "        generative_loss: GenerativeLoss,\n",
    "        intermediate_layers: Sequence[int],\n",
    "        learning_rate: float,\n",
    "        patience: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = VariationalAutoencoder(\n",
    "            reconstruction_loss, generative_loss, intermediate_layers, dropout=dropout\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.patience = patience\n",
    "        self.dropout = dropout\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.model(batch, compute_loss=True)\n",
    "        output.log(self, batch, \"train\")\n",
    "        return output.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.model(batch, compute_loss=True)\n",
    "        output.log(self, batch, \"val\")\n",
    "        return output.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": ReduceLROnPlateau(\n",
    "                optimizer, mode=\"max\", patience=self.patience, min_lr=1e-6\n",
    "            ),\n",
    "            \"monitor\": \"val_concordance\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdrsData(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(path, sep=\"\\t\").sort_values([\"PatientID\", \"Age\"])\n",
    "        measurements = (\n",
    "            data[[column for column in data.columns if column.startswith(\"3.\")]]\n",
    "            .dropna()\n",
    "            .astype(int)\n",
    "        )\n",
    "        self.covariates = data.loc[\n",
    "            measurements.index,\n",
    "            [\n",
    "                \"PatientID\",\n",
    "                \"Age\",\n",
    "                \"Deep brain stimulation available\",\n",
    "                \"Deep brain stimulation\",\n",
    "                \"Medication\",\n",
    "            ],\n",
    "        ].reset_index(drop=True)\n",
    "        self.measurements = torch.tensor(measurements.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.measurements[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.measurements)\n",
    "\n",
    "    @property\n",
    "    def participant_covariate(self) -> str:\n",
    "        return \"PatientID\"\n",
    "\n",
    "\n",
    "class UpdrsDataQoL(Dataset):\n",
    "    COLUMNS = [f\"UPDRS 1.{i}\" for i in range(1, 14)] + [\n",
    "        f\"UPDRS 2.{i}\" for i in range(1, 14)\n",
    "    ]\n",
    "\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(path, sep=\",\").sort_values([\"Participant\", \"Age\"])\n",
    "        measurements = data[UpdrsDataQoL.COLUMNS].dropna().astype(int)\n",
    "        self.covariates = data.loc[\n",
    "            measurements.index,\n",
    "            [\"Participant\", \"Age\"],\n",
    "        ].reset_index(drop=True)\n",
    "        self.measurements = torch.tensor(measurements.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.measurements[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.measurements)\n",
    "\n",
    "    @property\n",
    "    def participant_covariate(self) -> str:\n",
    "        return \"Participant\"\n",
    "\n",
    "\n",
    "class UpdrsDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        percentage_subjects_in_valid_dataset: float,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert 0 < percentage_subjects_in_valid_dataset <= 1\n",
    "\n",
    "        self.data = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if percentage_subjects_in_valid_dataset < 1:\n",
    "            patients = self.data.covariates[dataset.participant_covariate].unique()\n",
    "            num_patients_valid = int(\n",
    "                len(patients) * percentage_subjects_in_valid_dataset\n",
    "            )\n",
    "            first_patient_valid = len(patients) - num_patients_valid\n",
    "            # Find the index of the first patient in valid set\n",
    "            self.val_start = self.data.covariates[\n",
    "                self.data.covariates[dataset.participant_covariate]\n",
    "                == patients[first_patient_valid]\n",
    "            ].index[0]\n",
    "        else:\n",
    "            self.val_start = 0\n",
    "\n",
    "    def calculate_class_weights(self):\n",
    "        return torch.tensor(\n",
    "            compute_class_weight(\n",
    "                \"balanced\",\n",
    "                classes=range(5),\n",
    "                y=self.data.measurements[: self.val_start].flatten().long().numpy(),\n",
    "            )\n",
    "        ).float()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.val_start == 0:\n",
    "            raise ValueError(\"Only the validation set is used.\")\n",
    "        return DataLoader(\n",
    "            Subset(self.data, range(0, self.val_start)),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            Subset(self.data, range(self.val_start, len(self.data))),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "\n",
    "data = UpdrsDataQoL(\n",
    "    \"/workspaces/de.uke.iam.parkinson.vae_longitudinal/data/pdq_amp.csv\"\n",
    ")\n",
    "data_module = UpdrsDataModule(\n",
    "    data,\n",
    "    percentage_subjects_in_valid_dataset=0.2,\n",
    "    batch_size=512,\n",
    ")\n",
    "print(len(data_module.train_dataloader()))\n",
    "print(len(data_module.val_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"updrs_qol_vae_new\"\n",
    "\n",
    "reconstruction_loss = MultiCategoricalLoss(\n",
    "    n_values=len(UpdrsDataQoL.COLUMNS),\n",
    "    n_classes=5,\n",
    "    is_categorical=False,\n",
    "    is_ordinal=True,\n",
    "    weight=data_module.calculate_class_weights().to(\"cuda\"),\n",
    ")\n",
    "#generative_loss = KullbackLeiblerLoss(beta=1.0)\n",
    "generative_loss = WassersteinLoss(reg_weight=100, kernel_type=\"imq\", z_var=2.0)\n",
    "model = VariationAutoencoderModule(\n",
    "    reconstruction_loss, generative_loss, [64, 48, 32, 16], patience=80, learning_rate=1e-3, dropout=0.05\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=NAME,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_concordance\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_concordance\", patience=120, mode=\"max\")\n",
    "logger = TensorBoardLogger(\"logs\", name=NAME)\n",
    "\n",
    "# Initialize the PyTorch Lightning trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1000, callbacks=[early_stopping], logger=logger, log_every_n_steps=6\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariationAutoencoderModule.load_from_checkpoint(\n",
    "    \"/workspaces/de.uke.iam.parkinson.vae_longitudinal/src/logs/updrs_qol_vae/version_1/checkpoints/epoch=431-step=2592.ckpt\"\n",
    ").model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "def load_testdata(path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    data = pd.read_csv(\n",
    "        path,\n",
    "        sep=\",\",\n",
    "        na_values=[\n",
    "            \"Keine_Angabe\",\n",
    "            \"Nicht_durchgef√ºhrt\",\n",
    "            \"Keine_Angaben\",\n",
    "            \"Keine_angabe\",\n",
    "        ],\n",
    "    )\n",
    "    pdq_columns = [column for column in data.columns if column.startswith(\"PDQ39 \")]\n",
    "    updrs_columns = [\n",
    "        column\n",
    "        for column in data.columns\n",
    "        if column.startswith(\"UPDRS 1.\") or column.startswith(\"UPDRS 2.\")\n",
    "    ]\n",
    "    data = data[pdq_columns + updrs_columns].dropna().reset_index(drop=True)\n",
    "    return data[updrs_columns], data[pdq_columns]\n",
    "\n",
    "\n",
    "test_updrs, test_pdq = load_testdata(\n",
    "    \"/workspaces/de.uke.iam.parkinson.vae_longitudinal/data/pdq_uke_new.csv\"\n",
    ")\n",
    "\n",
    "ground_truth = next(iter(data_module.val_dataloader()))\n",
    "ground_truth_sum = ground_truth.sum(axis=-1).to(\"cuda\")\n",
    "reconstruction_sum = model(ground_truth.to(\"cuda\")).x_recon.sum(axis=-1)\n",
    "\n",
    "print(\n",
    "    F.l1_loss(\n",
    "        input=ground_truth.to(\"cuda\"),\n",
    "        target=model(ground_truth.to(\"cuda\")).x_recon,\n",
    "        reduction=\"none\",\n",
    "    )\n",
    "    .sum(axis=-1)\n",
    "    .mean()\n",
    "    / (ground_truth.shape[-1])\n",
    ")\n",
    "print(\n",
    "    (\n",
    "        (ground_truth.to(\"cuda\") != model(ground_truth.to(\"cuda\")).x_recon).sum(axis=-1)\n",
    "        / ground_truth.shape[-1]\n",
    "    ).median()\n",
    ")\n",
    "print(\n",
    "    torchmetrics.functional.concordance_corrcoef(\n",
    "        target=ground_truth_sum.to(torch.float),\n",
    "        preds=reconstruction_sum.to(torch.float),\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
