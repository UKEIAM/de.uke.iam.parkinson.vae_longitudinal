{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from coral_pytorch.losses import corn_loss\n",
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "from torchmetrics.functional import concordance_corrcoef\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class MultiCategoricalLoss:\n",
    "    n_values: int\n",
    "    n_classes: int\n",
    "    is_categorical: bool\n",
    "    is_ordinal: bool\n",
    "    weight: torch.Tensor = None\n",
    "    beta: float = 1.0\n",
    "    \n",
    "    @property\n",
    "    def last_layer_size(self) -> int:\n",
    "        if self.is_categorical and self.is_ordinal:\n",
    "            raise ValueError(\"Cannot be both categorical and ordinal.\")\n",
    "        \n",
    "        if self.is_categorical:\n",
    "            return self.n_values * self.n_classes\n",
    "        elif self.is_ordinal:\n",
    "            return self.n_values * (self.n_classes - 1)\n",
    "        else:\n",
    "            return self.n_values\n",
    "\n",
    "    def calculate_reconstruction(self, y_raw: torch.Tensor) -> torch.LongTensor:\n",
    "        if self.is_categorical:\n",
    "            return y_raw.reshape((-1, self.n_values, self.n_classes)).argmax(dim=-1)\n",
    "        elif self.is_ordinal:\n",
    "            return corn_label_from_logits(y_raw.reshape(-1, self.n_classes - 1)).reshape((-1, self.n_values))\n",
    "        else:\n",
    "            return (F.sigmoid(y_raw) * (self.n_classes - 1)).round().long()\n",
    "\n",
    "    def calculate_reconstruction_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "        if self.is_categorical:\n",
    "            y_pred = y_pred.reshape((-1, self.n_values, self.n_classes))\n",
    "            return F.cross_entropy(\n",
    "                input=y_pred.reshape(-1, self.n_classes),\n",
    "                target=y_true.reshape(-1).long(),\n",
    "                weight=self.weight,\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        elif self.is_ordinal:\n",
    "            y_pred = y_pred.reshape((-1, self.n_values, self.n_classes - 1))\n",
    "            return corn_loss(\n",
    "                logits=y_pred.reshape(-1, self.n_classes - 1),\n",
    "                y_train=y_true.reshape(-1).long(),\n",
    "                num_classes=self.n_classes,\n",
    "            )\n",
    "        else:\n",
    "            return F.mse_loss(\n",
    "                input=F.sigmoid(y_pred),\n",
    "                target=y_true / (self.n_classes - 1),\n",
    "                reduction=\"none\",\n",
    "            ).sum(axis=-1).mean()\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class VariationalAutoencoderOutput:\n",
    "    z_dist: torch.distributions.Distribution\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.LongTensor\n",
    "\n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_kl: torch.Tensor\n",
    "\n",
    "    def log(self, logger, batch, prefix: str):\n",
    "        logger.log(f\"{prefix}_loss\", self.loss)\n",
    "        logger.log(f\"{prefix}_loss_recon\", self.loss_recon)\n",
    "        logger.log(f\"{prefix}_loss_kl\", self.loss_kl)\n",
    "\n",
    "        # Log evaluation metrices\n",
    "        logger.log(f\"{prefix}_rel_wrong_items\",  ((batch != self.x_recon).sum(axis=-1) / batch.shape[-1]).median())\n",
    "        real_sum = batch.sum(axis=-1)\n",
    "        reconstructed_sum = self.x_recon.sum(axis=-1)\n",
    "        logger.log(f\"{prefix}_concordance\", concordance_corrcoef(target=real_sum.to(torch.float), preds=reconstructed_sum.to(torch.float)))\n",
    "\n",
    "    def calculate_total_error(self, batch):\n",
    "        real_sum = batch.sum(axis=-1)\n",
    "        reconstructed_sum = self.x_recon.sum(axis=-1)\n",
    "        return F.l1_loss(input=reconstructed_sum, target=real_sum, reduction=\"mean\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: Sequence[int]):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i, (i_size, out_size) in enumerate(zip(layers, layers[1:])):\n",
    "            if i < len(layers) - 2:\n",
    "                self.encoder.append(nn.Linear(i_size, out_size))\n",
    "                self.encoder.append(nn.Dropout(0.1))\n",
    "                self.encoder.append(nn.SiLU())\n",
    "            else:\n",
    "                self.encoder.append(nn.Linear(i_size, 2 * out_size)) # 2 for mean and variance.\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x, eps: float = 1e-8) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        return mu, scale\n",
    "        \n",
    "    def forward_and_reparameterize(self, x) -> Tuple[torch.distributions.Distribution, torch.Tensor]:\n",
    "        mu, scale = self(x)\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        dist = torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "        return dist, dist.rsample()\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: Sequence[int]):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential()\n",
    "        for i, (i_size, out_size) in enumerate(zip(layers, layers[1:])):\n",
    "            self.decoder.append(nn.Linear(i_size, out_size))\n",
    "            if i < len(layers) - 2:\n",
    "                self.decoder.append(nn.Dropout(0.1))\n",
    "                self.decoder.append(nn.SiLU())\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, loss: MultiCategoricalLoss, intermediate_layers: Sequence[int]):\n",
    "        super().__init__()\n",
    "        self.loss = loss\n",
    "        self.encoder = Encoder([loss.n_values] + intermediate_layers)\n",
    "        self.decoder = Decoder(list(reversed(intermediate_layers)) + [loss.last_layer_size])\n",
    "\n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        dist, z = self.encoder.forward_and_reparameterize(x)\n",
    "        recon_x = self.decoder(z)\n",
    "\n",
    "        if not compute_loss:\n",
    "            return VariationalAutoencoderOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=self.loss.calculate_reconstruction(recon_x),\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "\n",
    "        loss_recon = self.loss.calculate_reconstruction_loss(y_true=x, y_pred=recon_x)\n",
    "        std_normal = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros_like(z, device=z.device),\n",
    "            scale_tril=torch.eye(z.shape[-1], device=z.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(z.shape[0], -1, -1),\n",
    "        )\n",
    "        loss_kl = torch.distributions.kl.kl_divergence(dist, std_normal).mean()\n",
    "\n",
    "        loss = loss_recon + self.loss.beta * loss_kl\n",
    "        return VariationalAutoencoderOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=self.loss.calculate_reconstruction(recon_x),\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )\n",
    "\n",
    "\n",
    "class VariationAutoencoderModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss: MultiCategoricalLoss,\n",
    "        intermediate_layers: Sequence[int],\n",
    "        learning_rate: float = 1e-4,\n",
    "        patience: int = 10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = VariationalAutoencoder(loss, intermediate_layers)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.patience = patience\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.model(batch, compute_loss=True)\n",
    "        output.log(self, batch, \"train\")\n",
    "        return output.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.model(batch, compute_loss=True)\n",
    "        output.log(self, batch, \"val\")\n",
    "        return output.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": ReduceLROnPlateau(\n",
    "                optimizer, mode=\"min\", patience=self.patience\n",
    "            ),\n",
    "            \"monitor\": \"val_rel_wrong_items\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdrsData(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(path, sep=\"\\t\").sort_values([\"PatientID\", \"Age\"])\n",
    "        measurements = (\n",
    "            data[[column for column in data.columns if column.startswith(\"3.\")]]\n",
    "            .dropna()\n",
    "            .astype(int)\n",
    "        )\n",
    "        self.covariates = data.loc[\n",
    "            measurements.index,\n",
    "            [\n",
    "                \"PatientID\",\n",
    "                \"Age\",\n",
    "                \"Deep brain stimulation available\",\n",
    "                \"Deep brain stimulation\",\n",
    "                \"Medication\",\n",
    "            ],\n",
    "        ].reset_index(drop=True)\n",
    "        self.measurements = torch.tensor(measurements.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.measurements[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.measurements)\n",
    "\n",
    "    @property\n",
    "    def participant_covariate(self) -> str:\n",
    "        return \"PatientID\"\n",
    "    \n",
    "class UpdrsDataQoL(Dataset):\n",
    "    COLUMNS = [f\"UPDRS 1.{i}\" for i in range(1, 14)] + [f\"UPDRS 2.{i}\" for i in range(1, 14)]\n",
    "\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(path, sep=\",\").sort_values([\"Participant\", \"Age\"])\n",
    "        measurements = (\n",
    "            data[UpdrsDataQoL.COLUMNS]\n",
    "            .dropna()\n",
    "            .astype(int)\n",
    "        )\n",
    "        self.covariates = data.loc[\n",
    "            measurements.index,\n",
    "            [\n",
    "                \"Participant\",\n",
    "                \"Age\"\n",
    "            ],\n",
    "        ].reset_index(drop=True)\n",
    "        self.measurements = torch.tensor(measurements.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.measurements[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.measurements)\n",
    "    \n",
    "    @property\n",
    "    def participant_covariate(self) -> str:\n",
    "        return \"Participant\"\n",
    "\n",
    "class UpdrsDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, dataset: Dataset, percentage_subjects_in_valid_dataset: float, batch_size: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert 0 < percentage_subjects_in_valid_dataset <= 1\n",
    "\n",
    "        self.data = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if percentage_subjects_in_valid_dataset < 1:\n",
    "            patients = self.data.covariates[dataset.participant_covariate].unique()\n",
    "            num_patients_valid = int(len(patients) * percentage_subjects_in_valid_dataset)\n",
    "            first_patient_valid = len(patients) - num_patients_valid\n",
    "            # Find the index of the first patient in valid set\n",
    "            self.val_start = self.data.covariates[\n",
    "                self.data.covariates[dataset.participant_covariate] == patients[first_patient_valid]\n",
    "            ].index[0]\n",
    "        else:\n",
    "            self.val_start = 0\n",
    "\n",
    "    def calculate_class_weights(self):\n",
    "        return torch.tensor(\n",
    "            compute_class_weight(\n",
    "                \"balanced\",\n",
    "                classes=range(5),\n",
    "                y=self.data.measurements[: self.val_start].flatten().long().numpy(),\n",
    "            )\n",
    "        ).float()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.val_start == 0:\n",
    "            raise ValueError(\"Only the validation set is used.\")\n",
    "        return DataLoader(\n",
    "            Subset(self.data, range(0, self.val_start)),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            Subset(self.data, range(self.val_start, len(self.data))),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "data = UpdrsDataQoL(\"/workspaces/de.uke.iam.parkinson.vae_longitudinal/data/pdq_amp.csv\")\n",
    "data_module = UpdrsDataModule(\n",
    "    data,\n",
    "    percentage_subjects_in_valid_dataset=0.2,\n",
    "    batch_size=512,\n",
    ")\n",
    "print(len(data_module.train_dataloader()))\n",
    "print(len(data_module.val_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"updrs_qol_vae_new\"\n",
    "\n",
    "loss = MultiCategoricalLoss(n_values=len(UpdrsDataQoL.COLUMNS), n_classes=5, is_categorical=False, is_ordinal=True, weight=data_module.calculate_class_weights().to(\"cuda\"))\n",
    "model = VariationAutoencoderModule(loss, [64, 48, 32, 16], patience=80)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=NAME,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_rel_wrong_items\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_rel_wrong_items\", patience=120, mode=\"min\")\n",
    "logger = TensorBoardLogger(\"logs\", name=NAME)\n",
    "\n",
    "# Initialize the PyTorch Lightning trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1000, callbacks=[early_stopping], logger=logger, log_every_n_steps=6\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariationAutoencoderModule.load_from_checkpoint(\"/workspaces/de.uke.iam.parkinson.vae_longitudinal/src/logs/updrs_qol_vae/version_1/checkpoints/epoch=431-step=2592.ckpt\").model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def load_testdata(path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    data = pd.read_csv(path, sep=\",\", na_values=[\"Keine_Angabe\", \"Nicht_durchgef√ºhrt\", \"Keine_Angaben\", \"Keine_angabe\"])\n",
    "    pdq_columns = [column for column in data.columns if column.startswith(\"PDQ39 \")]\n",
    "    updrs_columns = [column for column in data.columns if column.startswith(\"UPDRS 1.\") or column.startswith(\"UPDRS 2.\")]\n",
    "    data = data[pdq_columns + updrs_columns].dropna().reset_index(drop=True)\n",
    "    return data[updrs_columns], data[pdq_columns]\n",
    "\n",
    "test_updrs, test_pdq = load_testdata(\"/workspaces/de.uke.iam.parkinson.vae_longitudinal/data/pdq_uke_new.csv\")\n",
    "\n",
    "ground_truth = next(iter(data_module.val_dataloader()))\n",
    "ground_truth_sum = ground_truth.sum(axis=-1).to(\"cuda\")\n",
    "reconstruction_sum = model(ground_truth.to(\"cuda\")).x_recon.sum(axis=-1)\n",
    "\n",
    "print(F.l1_loss(input=ground_truth.to(\"cuda\"), target=model(ground_truth.to(\"cuda\")).x_recon, reduction=\"none\").sum(axis=-1).mean() / (ground_truth.shape[-1]))\n",
    "print(((ground_truth.to(\"cuda\") != model(ground_truth.to(\"cuda\")).x_recon).sum(axis=-1) / ground_truth.shape[-1]).median())\n",
    "print(torchmetrics.functional.concordance_corrcoef(target=ground_truth_sum.to(torch.float), preds=reconstruction_sum.to(torch.float)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
