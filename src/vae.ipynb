{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from coral_pytorch.losses import corn_loss\n",
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class MultiCategoricalLoss:\n",
    "    n_values: int\n",
    "    n_classes: int\n",
    "    is_categorical: bool\n",
    "    is_ordinal: bool\n",
    "    weight: torch.Tensor = None\n",
    "\n",
    "    @property\n",
    "    def last_layer_size(self) -> int:\n",
    "        if self.is_categorical and self.is_ordinal:\n",
    "            raise ValueError(\"Cannot be both categorical and ordinal.\")\n",
    "        \n",
    "        if self.is_categorical:\n",
    "            return self.n_values * self.n_classes\n",
    "        elif self.is_ordinal:\n",
    "            return self.n_values * (self.n_classes - 1)\n",
    "        else:\n",
    "            return self.n_values\n",
    "\n",
    "    def calculate_reconstruction(self, y_raw: torch.Tensor) -> torch.LongTensor:\n",
    "        if self.is_categorical:\n",
    "            return y_raw.view((-1, self.n_values, self.n_classes)).argmax(dim=-1)\n",
    "        elif self.is_ordinal:\n",
    "            return corn_label_from_logits(y_raw.view(-1, self.n_classes - 1)).view((-1, self.n_values))\n",
    "        else:\n",
    "            return (F.sigmoid(y_raw) * (self.n_classes - 1)).round().long()\n",
    "\n",
    "    def calculate_reconstruction_loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "        if self.is_categorical:\n",
    "            y_pred = y_pred.view((-1, self.n_values, self.n_classes))\n",
    "            return F.cross_entropy(\n",
    "                input=y_pred.view(-1, self.n_classes),\n",
    "                target=y_true.view(-1).long(),\n",
    "                weight=self.weight,\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        elif self.is_ordinal:\n",
    "            y_pred = y_pred.view((-1, self.n_values, self.n_classes - 1))\n",
    "            return corn_loss(\n",
    "                logits=y_pred.view(-1, self.n_classes - 1),\n",
    "                y_train=y_true.view(-1).long(),\n",
    "                num_classes=self.n_classes,\n",
    "            )\n",
    "        else:\n",
    "            return F.mse_loss(\n",
    "                input=F.sigmoid(y_pred),\n",
    "                target=y_true / (self.n_classes - 1),\n",
    "                reduction=\"none\",\n",
    "            ).sum(axis=-1).mean()\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class VariationalAutoencoderOutput:\n",
    "    z_dist: torch.distributions.Distribution\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.LongTensor\n",
    "\n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_kl: torch.Tensor\n",
    "\n",
    "    def log(self, logger, prefix: str):\n",
    "        logger.log(f\"{prefix}_loss\", self.loss)\n",
    "        logger.log(f\"{prefix}_loss_recon\", self.loss_recon)\n",
    "        logger.log(f\"{prefix}_loss_kl\", self.loss_kl)\n",
    "\n",
    "    def calculate_total_error(self, batch):\n",
    "        real_sum = batch.sum(axis=-1)\n",
    "        reconstructed_sum = self.x_recon.sum(axis=-1)\n",
    "        return F.l1_loss(input=reconstructed_sum, target=real_sum, reduction=\"mean\")\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, loss: MultiCategoricalLoss, hidden_dim: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.loss = loss\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(loss.n_values, loss.n_values),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(loss.n_values, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 2, 2 * latent_dim),  # 2 for mean and variance.\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, loss.last_layer_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(loss.last_layer_size, loss.last_layer_size),\n",
    "        )\n",
    "\n",
    "    def encode(self, x, eps: float = 1e-8):\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        return torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "\n",
    "    def reparameterize(self, dist):\n",
    "        return dist.rsample()\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, compute_loss: bool = True, weight: torch.Tensor = None):\n",
    "        dist = self.encode(x)\n",
    "        z = self.reparameterize(dist)\n",
    "        recon_x = self.decode(z)\n",
    "\n",
    "        if not compute_loss:\n",
    "            return VariationalAutoencoderOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=self.loss.calculate_reconstruction(recon_x),\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "\n",
    "        loss_recon = self.loss.calculate_reconstruction_loss(y_true=x, y_pred=recon_x)\n",
    "        std_normal = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros_like(z, device=z.device),\n",
    "            scale_tril=torch.eye(z.shape[-1], device=z.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(z.shape[0], -1, -1),\n",
    "        )\n",
    "        loss_kl = torch.distributions.kl.kl_divergence(dist, std_normal).mean()\n",
    "\n",
    "        loss = loss_recon + loss_kl\n",
    "        return VariationalAutoencoderOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=self.loss.calculate_reconstruction(recon_x),\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )\n",
    "\n",
    "\n",
    "class VariationAutoencoderModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss: MultiCategoricalLoss,\n",
    "        learning_rate: float = 1e-4,\n",
    "        patience: int = 10,\n",
    "        latent_dim: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = VariationalAutoencoder(loss, 24, latent_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.patience = patience\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.model(batch, compute_loss=True)\n",
    "        output.log(self, \"train\")\n",
    "        self.log(\"train_l1_error\", output.calculate_total_error(batch))\n",
    "        return output.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.model(batch, compute_loss=True)\n",
    "        output.log(self, \"val\")\n",
    "        self.log(\"val_l1_error\", output.calculate_total_error(batch))\n",
    "        return output.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": ReduceLROnPlateau(\n",
    "                optimizer, mode=\"min\", patience=self.patience\n",
    "            ),\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdrsData(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(path, sep=\"\\t\").sort_values([\"PatientID\", \"Age\"])\n",
    "        measurements = (\n",
    "            data[[column for column in data.columns if column.startswith(\"3.\")]]\n",
    "            .dropna()\n",
    "            .astype(int)\n",
    "        )\n",
    "        self.covariates = data.loc[\n",
    "            measurements.index,\n",
    "            [\n",
    "                \"PatientID\",\n",
    "                \"Age\",\n",
    "                \"Deep brain stimulation available\",\n",
    "                \"Deep brain stimulation\",\n",
    "                \"Medication\",\n",
    "            ],\n",
    "        ].reset_index(drop=True)\n",
    "        self.measurements = torch.tensor(measurements.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.measurements[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.measurements)\n",
    "\n",
    "\n",
    "class UpdrsDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, path: str, percentage_subjects_in_valid_dataset: float, batch_size: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert 0 < percentage_subjects_in_valid_dataset <= 1\n",
    "\n",
    "        self.data = UpdrsData(path)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if percentage_subjects_in_valid_dataset < 1:\n",
    "            patients = self.data.covariates[\"PatientID\"].unique()\n",
    "            num_patients_valid = int(len(patients) * percentage_subjects_in_valid_dataset)\n",
    "            first_patient_valid = len(patients) - num_patients_valid\n",
    "            # Find the index of the first patient in valid set\n",
    "            self.val_start = self.data.covariates[\n",
    "                self.data.covariates[\"PatientID\"] == patients[first_patient_valid]\n",
    "            ].index[0]\n",
    "        else:\n",
    "            self.val_start = 0\n",
    "\n",
    "    def calculate_class_weights(self):\n",
    "        return torch.tensor(\n",
    "            compute_class_weight(\n",
    "                \"balanced\",\n",
    "                classes=range(5),\n",
    "                y=self.data.measurements[: self.val_start].flatten().long().numpy(),\n",
    "            )\n",
    "        ).float()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.val_start == 0:\n",
    "            raise ValueError(\"Only the validation set is used.\")\n",
    "        return DataLoader(\n",
    "            Subset(self.data, range(0, self.val_start)),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            Subset(self.data, range(self.val_start, len(self.data))),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "\n",
    "data_module = UpdrsDataModule(\n",
    "    \"../data/ppmi_with_meta.csv\",\n",
    "    percentage_subjects_in_valid_dataset=0.3,\n",
    "    batch_size=512,\n",
    ")\n",
    "print(len(data_module.train_dataloader()))\n",
    "print(len(data_module.val_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"vanilla_vae\"\n",
    "\n",
    "loss = MultiCategoricalLoss(n_values=33, n_classes=5, is_categorical=False, is_ordinal=True)\n",
    "model = VariationAutoencoderModule(loss, latent_dim=6)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=NAME,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\")\n",
    "logger = TensorBoardLogger(\"logs\", name=NAME)\n",
    "\n",
    "# Initialize the PyTorch Lightning trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1000, callbacks=[early_stopping], logger=logger, log_every_n_steps=25\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VariationAutoencoderModule.load_from_checkpoint(\"/workspaces/de.uke.iam.parkinson.vae_longitudinal/src/logs/vanilla_vae/version_4/checkpoints/epoch=324-step=8125.ckpt\").model\n",
    "model = model.model\n",
    "\n",
    "model.eval()\n",
    "data_module = UpdrsDataModule(\n",
    "    \"../data/uke_with_meta.csv\",\n",
    "    percentage_subjects_in_valid_dataset=1.0,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "batch = next(iter(data_module.val_dataloader()))\n",
    "ground_truth = batch[0].numpy().astype(int)\n",
    "prediction = model.model(batch).x_recon[0].detach().numpy()\n",
    "\n",
    "print(ground_truth)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
